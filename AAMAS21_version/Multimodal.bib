Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{liang2017,
abstract = {Existing inefficient traffic light cycle control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. Existing works either split the traffic signal into equal duration or only leverage limited traffic information. In this paper, we study how to decide the traffic signal duration based on the collected data from different sensors. We propose a deep reinforcement learning model to control the traffic light cycle. In the model, we quantify the complex traffic scenario as states by collecting traffic data and dividing the whole intersection into small grids. The duration changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map states to rewards. The proposed model incorporates multiple optimization elements to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation on a Simulation of Urban MObility simulator. Simulation results show the efficiency of our model in controlling traffic lights.},
author = {Liang, Xiaoyuan and Du, Xunsheng and Wang, Guiling and Han, Zhu},
doi = {10.1109/TVT.2018.2890726},
issn = {00189545},
journal = {IEEE Transactions on Vehicular Technology},
keywords = {Reinforcement learning,deep learning,traffic light control,vehicular network},
number = {2},
pages = {1243--1253},
publisher = {IEEE},
title = {{A Deep Reinforcement Learning Network for Traffic Light Cycle Control}},
volume = {68},
year = {2019}
}
@inproceedings{abdoos2011,
author = {Abdoos, Monireh and Mozayani, Nasser and Bazzan, Ana L C},
booktitle = {2011 14th International IEEE conference on intelligent transportation systems (ITSC)},
organization = {IEEE},
pages = {1580--1585},
title = {{Traffic light control in non-stationary environments based on multi agent Q-learning}},
year = {2011}
}
@inproceedings{previous,
author = {{Cabrejas Egea}, Alvaro and Howell, Shaun and Knutins, Maksis and Connaughton, Colm},
booktitle = {IEEE International Conference on Systems, Man and Cybernetics},
title = {{Assessment of Reward Functions for Reinforcement Learning Traffic Signal Control under Real-World Limitations}},
year = {2020}
}
@article{wan2018,
author = {Wan, Chia-Hao and Hwang, Ming-Chorng},
journal = {IET Intelligent Transport Systems},
number = {9},
pages = {1005--1010},
publisher = {IET},
title = {{Value-based deep reinforcement learning for adaptive isolated intersection signal control}},
volume = {12},
year = {2018}
}
@article{chen2020,
abstract = {Traffic congestion plagues cities around the world. Recent years have witnessed an unprecedented trend in applying reinforcement learning for traffic signal control. However, the primary challenge is to control and coordinate traffic lights in large-scale urban networks. No one has ever tested RL models on a network of more than a thousand traffic lights. In this paper, we tackle the problem of multi-intersection traffic signal control, especially for large-scale networks, based on RL techniques and transportation theories. This problem is quite difficult because there are challenges such as scal-ability, signal coordination, data feasibility, etc. To address these challenges, we (1) design our RL agents utilizing 'pres-sure' concept to achieve signal coordination in region-level; (2) show that implicit coordination could be achieved by individual control agents with well-crafted reward design thus reducing the dimensionality; and (3) conduct extensive experiments on multiple scenarios, including a real-world scenario with 2510 traffic lights in Manhattan, New York City 1 2 .},
author = {Chen, Chacha and Wei, Hua and Xu, Nan and Zheng, Guanjie and Yang, Ming and Xiong, Yuanhao and Xu, Kai and Li, Zhenhui},
journal = {AAAI},
pages = {3414--3121},
title = {{Toward A Thousand Lights: Decentralized Deep Reinforcement Learning for Large-Scale Traffic Signal Control}},
url = {www.aaai.org},
year = {2020}
}
@article{survey2020wei,
author = {Wei, Hua and Zheng, Guanjie and Gayah, Vikash and Li, Zhenhui},
journal = {arXiv preprint arXiv:1904.08117},
title = {{A survey on traffic signal control methods}},
year = {2019}
}
@inproceedings{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Others},
booktitle = {Advances in neural information processing systems},
pages = {8026--8037},
title = {{Pytorch: An imperative style, high-performance deep learning library}},
year = {2019}
}
@article{gao2017,
author = {Gao, Juntao and Shen, Yulong and Liu, Jia and Ito, Minoru and Shiratori, Norio},
journal = {arXiv preprint arXiv:1705.02755},
title = {{Adaptive traffic signal control: Deep reinforcement learning algorithm with experience replay and target network}},
year = {2017}
}
@article{pra2010,
author = {Prashanth, L A and Bhatnagar, Shalabh},
journal = {IEEE Transactions on Intelligent Transportation Systems},
number = {2},
pages = {412--421},
publisher = {IEEE},
title = {{Reinforcement learning with function approximation for traffic signal control}},
volume = {12},
year = {2010}
}
@article{liu2017,
abstract = {The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints / rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.},
archivePrefix = {arXiv},
arxivId = {1711.10941},
author = {Liu, Ying and Liu, Lei and Chen, Wei Peng},
doi = {10.1109/ITSC.2017.8317730},
eprint = {1711.10941},
file = {:C$\backslash$:/Users/ACE-Desktop/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Liu, Chen - 2017 - Intelligent Traffic Light Control Using Distributed Multi-agent Q Learning.pdf:pdf},
isbn = {9781538615256},
issn = {2153-0009},
journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
keywords = {Q learning,Reinforcement learning,non-motorized traffic,traffic light control},
pages = {1--8},
title = {{Intelligent traffic light control using distributed multi-agent Q learning}},
url = {http://arxiv.org/abs/1711.10941},
volume = {2018-March},
year = {2018}
}
@article{genders2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01142v1},
author = {Genders, Wade and Razavi, Saiedeh},
eprint = {arXiv:1611.01142v1},
pages = {1--9},
title = {{Using a Deep Reinforcement Learning Agent for Traffic Signal Control}},
year = {2016}
}
@article{survey2014,
author = {El-Tantawy, Samah and Abdulhai, Baher and Abdelgawad, Hossam},
journal = {Journal of Intelligent Transportation Systems},
number = {3},
pages = {227--245},
publisher = {Taylor {\&} Francis},
title = {{Design of reinforcement learning parameters for seamless application of adaptive traffic signal control}},
volume = {18},
year = {2014}
}
@article{scoot,
author = {Hunt, P B and Robertson, D I and Bretherton, R D and Royle, M Cr},
journal = {Traffic Engineering {\&} Control},
number = {4},
title = {{The SCOOT on-line traffic signal optimisation technique}},
volume = {23},
year = {1982}
}
@article{survey2020,
author = {Haydari, Ammar and Yilmaz, Yasin},
journal = {arXiv preprint arXiv:2005.00935},
title = {{Deep Reinforcement Learning for Intelligent Transportation Systems: A Survey}},
year = {2020}
}
@article{melo,
author = {Melo, Francisco S},
journal = {Institute Of Systems and Robotics, Tech. Rep},
pages = {1--4},
title = {{Convergence of Q-learning: A simple proof}},
year = {2001}
}
@book{mova,
author = {Vincent, R A and Peirce, J R},
publisher = {Traffic Management Division, Traffic Group, Transport and Road Research},
title = {{'MOVA': Traffic Responsive, Self-optimising Signal Control for Isolated Intersections}},
year = {1988}
}
@article{yau,
author = {{Yau, K. L. A., Qadir, J., Khoo, H. L., Ling, M. H., {\&} Komisarczuk}, P.},
doi = {https://dl.acm.org/doi/pdf/10.1145/3068287},
file = {:C$\backslash$:/Users/ACE-Desktop/Downloads/3068287.pdf:pdf},
journal = {ACM Computing Surveys},
number = {3},
title = {{A Survey on Reinforcement Learning Models and Algorithms}},
volume = {50},
year = {2017}
}
@article{adam,
author = {Kingma, Diederik P and Ba, Jimmy},
journal = {arXiv preprint arXiv:1412.6980},
title = {{Adam: A method for stochastic optimization}},
year = {2014}
}
@article{liang2018,
abstract = {Existing inefficient traffic light control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. In terms of how to dynamically adjust traffic signals' duration, existing works either split the traffic signal into equal duration or extract limited traffic information from the real data. In this paper, we study how to decide the traffic signals' duration based on the collected data from different sensors and vehicular networks. We propose a deep reinforcement learning model to control the traffic light. In the model, we quantify the complex traffic scenario as states by collecting data and dividing the whole intersection into small grids. The timing changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map the states to rewards. The proposed model is composed of several components to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the simulation results show the efficiency of our model in controlling traffic lights.},
archivePrefix = {arXiv},
arxivId = {1803.11115},
author = {Liang, Xiaoyuan and Du, Xunsheng and Wang, Guiling and Han, Zhu},
eprint = {1803.11115},
file = {:C$\backslash$:/Users/ACE-Desktop/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2018 - Deep Reinforcement Learning for Traffic Light Control in Vehicular Networks.pdf:pdf},
isbn = {0387310738},
number = {Xx},
pages = {1--11},
title = {{Deep Reinforcement Learning for Traffic Light Control in Vehicular Networks}},
url = {http://arxiv.org/abs/1803.11115},
volume = {XX},
year = {2018}
}
@phdthesis{gendersthesis,
author = {Genders, Wade},
title = {{Deep reinforcement learning adaptive traffic signal control}},
year = {2018}
}
@article{genders2019,
abstract = {Ensuring transportation systems are efficient is a priority for modern society. Intersection traffic signal control can be modeled as a sequential decision-making problem. To learn how to make the best decisions, we apply reinforcement learning techniques with function approximation to train an adaptive traffic signal controller. We use the asynchronous n-step Q-learning algorithm with a two hidden layer artificial neural network as our reinforcement learning agent. A dynamic, stochastic rush hour simulation is developed to test the agent's performance. Compared against traditional loop detector actuated and linear Q-learning traffic signal control methods, our reinforcement learning model develops a superior control policy, reducing mean total delay by up 40{\%} without compromising throughput. However, we find our proposed model slightly increases delay for left turning vehicles compared to the actuated controller, as a consequence of the reward function, highlighting the need for an appropriate reward function which truly develops the desired policy.},
author = {Genders, Wade and Razavi, Saiedeh},
doi = {10.1080/15472450.2018.1491003},
issn = {15472442},
journal = {Journal of Intelligent Transportation Systems: Technology, Planning, and Operations},
keywords = {Artificial intelligence,intelligent transportation systems,neural networks,reinforcement learning,traffic signal controllers},
number = {4},
pages = {319--331},
publisher = {Taylor {\&} Francis},
title = {{Asynchronous n-step Q-learning adaptive traffic signal control}},
url = {https://doi.org/10.1080/15472450.2018.1491003},
volume = {23},
year = {2019}
}
@article{gendersstate,
author = {Genders, Wade and Razavi, Saiedeh},
journal = {Procedia computer science},
pages = {26--33},
publisher = {Elsevier},
title = {{Evaluating reinforcement learning state representations for adaptive traffic signal control}},
volume = {130},
year = {2018}
}
@article{watkins,
author = {Watkins, Christopher J C H and Dayan, Peter},
journal = {Machine learning},
number = {3-4},
pages = {279--292},
publisher = {Springer},
title = {{Q-learning}},
volume = {8},
year = {1992}
}
@inproceedings{wiering2000,
abstract = {Introduction:Cost-effectiveness analyses are important tools in efforts to prioritise interventions for obesity prevention. Modelling facilitates evaluation of multiple scenarios with varying assumptions. This study compares the cost-effectiveness of conservative scenarios for two commonly proposed policy-based interventions: front-of-pack 'traffic-light' nutrition labelling (traffic-light labelling) and a tax on unhealthy foods ('junk-food' tax).Methods:For traffic-light labelling, estimates of changes in energy intake were based on an assumed 10{\%} shift in consumption towards healthier options in four food categories (breakfast cereals, pastries, sausages and preprepared meals) in 10{\%} of adults. For the 'junk-food' tax, price elasticities were used to estimate a change in energy intake in response to a 10{\%} price increase in seven food categories (including soft drinks, confectionery and snack foods). Changes in population weight and body mass index by sex were then estimated based on these changes in population energy intake, along with subsequent impacts on disability-adjusted life years (DALYs). Associated resource use was measured and costed using pathway analysis, based on a health sector perspective (with some industry costs included). Costs and health outcomes were discounted at 3{\%}. The cost-effectiveness of each intervention was modelled for the 2003 Australian adult population.Results:Both interventions resulted in reduced mean weight (traffic-light labelling: 1.3 kg (95{\%} uncertainty interval (UI): 1.2; 1.4); 'junk-food' tax: 1.6 kg (95{\%} UI: 1.5; 1.7)); and DALYs averted (traffic-light labelling: 45 100 (95{\%} UI: 37 700; 60 100); 'junk-food' tax: 559 000 (95{\%} UI: 459 500; 676 000)). Cost outlays were AUD81 million (95{\%} UI: 44.7; 108.0) for traffic-light labelling and AUD18 million (95{\%} UI: 14.4; 21.6) for 'junk-food' tax. Cost-effectiveness analysis showed both interventions were 'dominant' (effective and cost-saving).Conclusion:Policy-based population-wide interventions such as traffic-light nutrition labelling and taxes on unhealthy foods are likely to offer excellent 'value for money' as obesity prevention measures.International Journal of Obesity advance online publication, 16 November 2010; doi:10.1038/ijo.2010.228.},
address = {San Francisco, CA, USA},
author = {Wiering, Marco},
booktitle = {Proc Intl Conf Machine Learning},
doi = {10.1038/ijo.2010.228},
isbn = {1558607072},
issn = {14765497},
number = {JANUARY 2000},
pages = {1151--1158},
pmid = {21079620},
publisher = {Morgan Kaufmann Publishers Inc.},
series = {ICML '00},
title = {{Multi-Agent Reinforcement Learning for Traffic Light Control}},
url = {http://igitur-archive.library.uu.nl/math/2007-0330-200425/wiering{\_}00{\_}multi.pdf},
year = {2000}
}
@article{kiefer,
author = {Kiefer, Jack and Wolfowitz, Jacob and Others},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {462--466},
publisher = {Institute of Mathematical Statistics},
title = {{Stochastic estimation of the maximum of a regression function}},
volume = {23},
year = {1952}
}
@article{abdul2003,
author = {Abdulhai, Baher and Pringle, Rob and Karakoulas, Grigoris J},
journal = {Journal of Transportation Engineering},
number = {3},
pages = {278--285},
publisher = {American Society of Civil Engineers},
title = {{Reinforcement learning for true adaptive traffic signal control}},
volume = {129},
year = {2003}
}
@article{mnih2015,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
volume = {518},
year = {2015}
}
@article{highways,
author = {{Highways Agency}},
file = {:C$\backslash$:/Users/ACE-Desktop/Downloads/MCE0108C.pdf:pdf},
isbn = {MCE 0108 Issue C},
number = {C},
title = {{Siting of Inductive Loops for Vehicle Detecting Equipments at Permanent Road Traffic Signal Installations}},
year = {2002}
}
@inproceedings{aslani2019,
author = {Aslani, Mohammad and Mesgari, Mohammad Saadi and Seipel, Stefan and Wiering, Marco},
booktitle = {Proceedings of the Institution of Civil Engineers-Transport},
number = {5},
organization = {Thomas Telford Ltd},
pages = {289--298},
title = {{Developing adaptive traffic signal control by actor--critic and direct exploration methods}},
volume = {172},
year = {2019}
}
@inproceedings{SUMO,
author = {Lopez, Pablo Alvarez and Behrisch, Michael and Bieker-Walz, Laura and Erdmann, Jakob and Fl{\"{o}}tter{\"{o}}d, Yun-Pang and Hilbrich, Robert and L{\"{u}}cken, Leonhard and Rummel, Johannes and Wagner, Peter and WieBner, Evamarie},
booktitle = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
organization = {IEEE},
pages = {2575--2582},
title = {{Microscopic traffic simulation using sumo}},
year = {2018}
}
@inproceedings{geneticped,
author = {Turky, Ayad M and Ahmad, M S and Yusoff, Mohd Zaliman Mohd and Hammad, Baraa T},
booktitle = {International Conference on Rough Sets and Knowledge Technology},
organization = {Springer},
pages = {512--519},
title = {{Using genetic algorithm for traffic light control system with a pedestrian crossing}},
year = {2009}
}
@article{abdulhai2010,
abstract = {Adaptive traffic signal control is a promising technique for alleviating traffic congestion. Reinforcement Learning (RL) has the potential to tackle the optimal traffic control problem for a single agent. However, the ultimate goal is to develop integrated traffic control for multiple intersections. Integrated traffic control can be efficiently achieved using decentralized controllers. Multi-Agent Reinforcement Learning (MARL) is an extension of RL techniques that makes it possible to decentralize multiple agents in a non-stationary environments. Most of the studies in the field of traffic signal control consider a stationary environment, an approach whose shortcomings are highlighted in this paper. A Q-Learning-based acyclic signal control system that uses a variable phasing sequence is developed. To investigate the appropriate state model for different traffic conditions, three models were developed, each with different state representation. The models were tested on a typical multiphase intersection to minimize the vehicle delay and were compared to the pre-timed control strategy as a benchmark. The Q-Learning control system consistently outperformed the widely used Webster pre-timed optimized signal control strategy under various traffic conditions.},
author = {El-Tantawy, Samah and Abdulhai, Baher},
doi = {10.1109/ITSC.2010.5625066},
file = {:C$\backslash$:/Users/ACE-Desktop/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El-Tantawy, Abdulhai - 2010 - An agent-based learning towards decentralized and coordinated traffic signal control.pdf:pdf},
isbn = {9781424476572},
issn = {2153-0009},
journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
pages = {665--670},
title = {{An agent-based learning towards decentralized and coordinated traffic signal control}},
year = {2010}
}
@book{suttonbarto,
author = {Sutton, Richard S and Barto, Andrew G},
publisher = {MIT press},
title = {{Reinforcement learning: An introduction}},
year = {2018}
}
@article{scats,
author = {Lowrie, P R},
title = {{Scats, sydney co-ordinated adaptive traffic system: A traffic responsive method of controlling urban traffic}},
year = {1990}
}
@article{mousavi2017,
author = {Mousavi, Seyed Sajad and Schukat, Michael and Howley, Enda},
journal = {IET Intelligent Transport Systems},
number = {7},
pages = {417--423},
publisher = {IET},
title = {{Traffic light control using deep policy-gradient and value-function-based reinforcement learning}},
volume = {11},
year = {2017}
}
@incollection{mannion,
author = {Mannion, Patrick and Duggan, Jim and Howley, Enda},
booktitle = {Autonomic road transport support systems},
pages = {47--66},
publisher = {Springer},
title = {{An experimental review of reinforcement learning algorithms for adaptive traffic signal control}},
year = {2016}
}
